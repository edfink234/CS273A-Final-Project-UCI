{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce1dc91",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c273ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (129012, 2048)\n",
      "y shape: (129012,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"qm9_fp_U0.csv\")\n",
    "\n",
    "# Columns:\n",
    "# 0 = U0 (target)\n",
    "# 1 = SMILES\n",
    "# 2+ = Fingerprints\n",
    "\n",
    "# Target\n",
    "y = df.iloc[:, 0].values.astype(float)\n",
    "\n",
    "# Drop SMILES (col 1)\n",
    "X = df.iloc[:, 2:].values.astype(float)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1291227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (103209, 2048)\n",
      "Val: (12901, 2048)\n",
      "Test: (12902, 2048)\n"
     ]
    }
   ],
   "source": [
    "# First: 80% / 20%\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Split the 20% temp into 10% val + 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,   # half of 20% → 10%\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Val:\", X_val.shape)\n",
    "print(\"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdf1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcde93",
   "metadata": {},
   "source": [
    "# Subset training\n",
    "Training on the full set was taking too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99194ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search subset: 5000 / 103209\n",
      "\n",
      "--- Searching kernel: rbf ---\n",
      "Total combos: 18 | Already done: 0 | Remaining: 18\n",
      "[rbf] Batch 1/1 (18 configs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   3 out of  18 | elapsed:  5.0min remaining: 25.0min\n",
      "[Parallel(n_jobs=20)]: Done   5 out of  18 | elapsed:  5.0min remaining: 13.0min\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  18 | elapsed:  5.0min remaining:  7.9min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  18 | elapsed:  5.0min remaining:  5.0min\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  18 | elapsed:  5.0min remaining:  3.2min\n",
      "[Parallel(n_jobs=20)]: Done  13 out of  18 | elapsed:  5.0min remaining:  1.9min\n",
      "[Parallel(n_jobs=20)]: Done  15 out of  18 | elapsed:  5.0min remaining:  1.0min\n",
      "[Parallel(n_jobs=20)]: Done  18 out of  18 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rbf] rbf, C=300, eps=0.005, gamma=scale | val RMSE=660.161951 | 300.5s\n",
      "[rbf] rbf, C=300, eps=0.005, gamma=0.001 | val RMSE=849.320152 | 301.2s\n",
      "[rbf] rbf, C=300, eps=0.005, gamma=0.0003 | val RMSE=972.532852 | 299.8s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=scale | val RMSE=660.161865 | 299.9s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=0.001 | val RMSE=849.320307 | 298.8s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=0.0003 | val RMSE=972.532836 | 300.9s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=scale | val RMSE=631.593856 | 301.1s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=0.001 | val RMSE=781.747675 | 299.3s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=0.0003 | val RMSE=906.970348 | 300.2s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=scale | val RMSE=631.593809 | 300.4s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=0.001 | val RMSE=781.747580 | 299.8s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=0.0003 | val RMSE=906.970243 | 300.7s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=scale | val RMSE=616.683708 | 300.6s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=0.001 | val RMSE=758.529204 | 298.7s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=0.0003 | val RMSE=859.864305 | 298.3s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=scale | val RMSE=616.683648 | 298.0s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=0.001 | val RMSE=758.529318 | 295.1s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=0.0003 | val RMSE=859.864431 | 298.7s\n",
      "Best rbf: val RMSE=616.683648 | params={'kernel': 'rbf', 'C': 900, 'epsilon': 0.01, 'gamma': 'scale'}\n",
      "\n",
      "=== Best per kernel (subset+PCA search) ===\n",
      "[rbf] RMSE_val=616.683648 | params={'kernel': 'rbf', 'C': 900, 'epsilon': 0.01, 'gamma': 'scale'}\n",
      "\n",
      "=== Best overall (subset+PCA search) ===\n",
      "Kernel: rbf\n",
      "Params: {'kernel': 'rbf', 'C': 900, 'epsilon': 0.01, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "# svr_multikernel_subset_pca.py\n",
    "import os, time, json, math\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "SUBSET_N = 5000                  # subset size for search (5k–10k is good)\n",
    "USE_PCA_FOR_SEARCH = False        # PCA speeds up the search phase\n",
    "USE_PCA_FOR_FINAL  = False       # True = keep PCA in the final model; False = refit without PCA\n",
    "PCA_COMPONENTS = 300             # fixed # comps; typical sweet spot for ECFP2048\n",
    "PCA_VAR_TARGET = None            # e.g., 0.99 to target 99% explained variance (ignored if PCA_COMPONENTS set)\n",
    "CACHE_MB = 2000                  # libsvm cache in MB\n",
    "\n",
    "param_grid_rbf = {\n",
    "    \"kernel\":  [\"rbf\"],\n",
    "    \"C\":       [300, 600, 900],         # push higher; 300 looked good\n",
    "    \"epsilon\": [0.005, 0.01],           # a bit tighter around best\n",
    "    \"gamma\":   [\"scale\", 1e-3, 3e-4],   # add mid-low between 1e-3 and 1e-4\n",
    "}\n",
    "\n",
    "param_grid_linear = {\n",
    "    \"kernel\":  [\"linear\"],\n",
    "    \"C\":       [10, 100, 300, 1000],    # cheap to try higher C\n",
    "    \"epsilon\": [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "param_grid_sigmoid = {\n",
    "    \"kernel\":  [\"sigmoid\"],\n",
    "    \"C\":       [10, 100, 300],          # small bump in capacity\n",
    "    \"epsilon\": [0.01, 0.1],\n",
    "    \"gamma\":   [\"scale\", 1e-3],         # keep compact; sigmoid is finicky\n",
    "}\n",
    "\n",
    "INCLUDE_POLY = False\n",
    "param_grid_poly = {\n",
    "    \"kernel\":  [\"poly\"],\n",
    "    \"degree\":  [2],\n",
    "    \"C\":       [10, 100],\n",
    "    \"epsilon\": [0.01],\n",
    "    \"gamma\":   [\"scale\"],\n",
    "}\n",
    "# ====================================================\n",
    "\n",
    "# Assumes these exist in memory already:\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ---------- Output dirs ----------\n",
    "out = Path(\"artifacts_svr\"); out.mkdir(exist_ok=True)\n",
    "logs_dir = out / \"logs\"; logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- Build training subset ----------\n",
    "if SUBSET_N is not None and SUBSET_N < len(X_train):\n",
    "    idx = np.random.choice(len(X_train), SUBSET_N, replace=False)\n",
    "    X_train_sub, y_train_sub = X_train[idx], y_train[idx]\n",
    "else:\n",
    "    X_train_sub, y_train_sub = X_train, y_train\n",
    "\n",
    "print(f\"Search subset: {len(X_train_sub)} / {len(X_train)}\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def param_row_key(params: dict) -> str:\n",
    "    return json.dumps(params, sort_keys=True)\n",
    "\n",
    "def append_row(path: Path, row: dict):\n",
    "    write_header = not path.exists()\n",
    "    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=write_header, index=False)\n",
    "\n",
    "def grid_to_param_list(grid: dict):\n",
    "    keys = list(grid.keys())\n",
    "    return [dict(zip(keys, vals)) for vals in product(*[grid[k] for k in keys])]\n",
    "\n",
    "def eval_one(params, X_tr, y_tr, X_va, y_va):\n",
    "    # Map flat params to SVR kwargs\n",
    "    svr_kwargs = dict(C=params[\"C\"], epsilon=params[\"epsilon\"], cache_size=CACHE_MB)\n",
    "    if \"kernel\" in params: svr_kwargs[\"kernel\"] = params[\"kernel\"]\n",
    "    if \"gamma\"  in params: svr_kwargs[\"gamma\"]  = params[\"gamma\"]\n",
    "    if \"degree\" in params: svr_kwargs[\"degree\"] = params[\"degree\"]\n",
    "\n",
    "    model = SVR(**svr_kwargs)\n",
    "    t0 = time.perf_counter()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred_tr  = model.predict(X_tr)\n",
    "    pred_val = model.predict(X_va)\n",
    "    secs = time.perf_counter() - t0\n",
    "    return {\n",
    "        **params,\n",
    "        \"rmse_train\": rmse(y_tr, pred_tr),\n",
    "        \"rmse_val\": rmse(y_va, pred_val),\n",
    "        \"mae_val\": mean_absolute_error(y_va, pred_val),\n",
    "        \"r2_val\": r2_score(y_va, pred_val),\n",
    "        \"secs\": secs\n",
    "    }\n",
    "\n",
    "# ---------- Optional PCA for SEARCH (fit once on subset) ----------\n",
    "if USE_PCA_FOR_SEARCH:\n",
    "    # Scale -> PCA on subset; transform subset + val\n",
    "    scaler_sub = StandardScaler().fit(X_train_sub.astype(np.float32))\n",
    "    Xtr_s = scaler_sub.transform(X_train_sub.astype(np.float32))\n",
    "    Xva_s = scaler_sub.transform(X_val.astype(np.float32))\n",
    "\n",
    "    if PCA_COMPONENTS is not None:\n",
    "        pca_sub = PCA(n_components=PCA_COMPONENTS, random_state=SEED).fit(Xtr_s)\n",
    "    else:\n",
    "        # choose by variance target\n",
    "        pca_probe = PCA(n_components=min(Xtr_s.shape[0], Xtr_s.shape[1]), random_state=SEED).fit(Xtr_s)\n",
    "        cume = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "        k = int(np.searchsorted(cume, PCA_VAR_TARGET if PCA_VAR_TARGET else 0.99) + 1)\n",
    "        pca_sub = PCA(n_components=k, random_state=SEED).fit(Xtr_s)\n",
    "        print(f\"[SEARCH PCA] Target variance={PCA_VAR_TARGET or 0.99:.2f}; components={k}; \"\n",
    "              f\"explained={cume[k-1]:.4f}\")\n",
    "\n",
    "    X_train_sub_search = pca_sub.transform(Xtr_s)\n",
    "    X_val_search       = pca_sub.transform(Xva_s)\n",
    "    print(f\"[SEARCH PCA] Using {pca_sub.n_components_} comps; \"\n",
    "          f\"explained={pca_sub.explained_variance_ratio_.sum():.4f}\")\n",
    "else:\n",
    "    X_train_sub_search = X_train_sub.astype(np.float32)\n",
    "    X_val_search       = X_val.astype(np.float32)\n",
    "\n",
    "y_train_sub_search = y_train_sub.astype(np.float32)\n",
    "y_val_search       = y_val.astype(np.float32)\n",
    "\n",
    "# ---------- Search spaces ----------\n",
    "search_spaces = {\n",
    "    \"rbf\": param_grid_rbf,\n",
    "    # \"linear\": param_grid_linear,\n",
    "    # \"sigmoid\": param_grid_sigmoid,\n",
    "}\n",
    "if INCLUDE_POLY:\n",
    "    search_spaces[\"poly\"] = param_grid_poly\n",
    "\n",
    "# ---------- Chunked, parallel, resumable search ----------\n",
    "best_per_kernel = {}\n",
    "combined_csv = out / \"val_search_results_subset_streaming_pca.csv\"\n",
    "combined_csv = out / \"val_search_results_subset_streaming_rbf.csv\"\n",
    "\n",
    "# start fresh for this run\n",
    "if combined_csv.exists():\n",
    "    combined_csv.unlink()\n",
    "\n",
    "\n",
    "n_jobs = os.cpu_count() or 4\n",
    "BATCH = max(2, n_jobs * 2)\n",
    "\n",
    "for kernel_name, grid in search_spaces.items():\n",
    "    print(f\"\\n--- Searching kernel: {kernel_name} ---\")\n",
    "    kernel_csv = logs_dir / f\"{kernel_name}_stream_pca.csv\"\n",
    "\n",
    "    plist = grid_to_param_list(grid)\n",
    "    done_keys = set()\n",
    "    if kernel_csv.exists():\n",
    "        try:\n",
    "            prev = pd.read_csv(kernel_csv)\n",
    "            for _, row in prev.iterrows():\n",
    "                params = {k: row[k] for k in grid.keys() if k in row}\n",
    "                done_keys.add(param_row_key(params))\n",
    "        except Exception:\n",
    "            pass\n",
    "    todo = [p for p in plist if param_row_key(p) not in done_keys]\n",
    "    total = len(plist); remaining = len(todo)\n",
    "    print(f\"Total combos: {total} | Already done: {total-remaining} | Remaining: {remaining}\")\n",
    "\n",
    "    for start in range(0, remaining, BATCH):\n",
    "        chunk = todo[start:start+BATCH]\n",
    "        print(f\"[{kernel_name}] Batch {start//BATCH + 1}/{math.ceil(remaining/BATCH)} \"\n",
    "              f\"({len(chunk)} configs)...\")\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=10, batch_size=1)(\n",
    "            delayed(eval_one)(\n",
    "                p,\n",
    "                X_train_sub_search, y_train_sub_search,\n",
    "                X_val_search,       y_val_search\n",
    "            ) for p in chunk\n",
    "        )\n",
    "\n",
    "        for row in results:\n",
    "            row_stream = dict(row); row_stream[\"kernel_group\"] = kernel_name\n",
    "            append_row(kernel_csv, kernel_row := row_stream)\n",
    "            append_row(combined_csv, kernel_row)\n",
    "            print(f\"[{kernel_name}] {row.get('kernel')}, C={row.get('C')}, \"\n",
    "                  f\"eps={row.get('epsilon')}, gamma={row.get('gamma','-')} | \"\n",
    "                  f\"val RMSE={row['rmse_val']:.6f} | {row['secs']:.1f}s\")\n",
    "\n",
    "    df_k = pd.read_csv(kernel_csv).sort_values(\"rmse_val\").reset_index(drop=True)\n",
    "    best_row = df_k.iloc[0].to_dict()\n",
    "    best_params = {k: best_row[k] for k in grid.keys()}\n",
    "    best_per_kernel[kernel_name] = {\"best_row\": best_row, \"best_params\": best_params}\n",
    "    print(f\"Best {kernel_name}: val RMSE={best_row['rmse_val']:.6f} | params={best_params}\")\n",
    "\n",
    "# ---------- Best overall ----------\n",
    "all_df = pd.read_csv(combined_csv)\n",
    "\n",
    "# drop any junk rows without kernel_group\n",
    "all_df = all_df[all_df[\"kernel_group\"].notna()]\n",
    "all_df = all_df[all_df[\"kernel_group\"].isin(search_spaces.keys())]\n",
    "all_df = all_df.sort_values(\"rmse_val\").reset_index(drop=True)\n",
    "\n",
    "if len(all_df) == 0:\n",
    "    raise RuntimeError(\"No valid rows in combined_csv after filtering; check logs/ directory.\")\n",
    "\n",
    "best_overall_row = all_df.iloc[0].to_dict()\n",
    "best_kernel_name = best_overall_row[\"kernel_group\"]\n",
    "best_overall_params = {k: best_overall_row[k] for k in search_spaces[best_kernel_name].keys()}\n",
    "\n",
    "print(\"\\n=== Best per kernel (subset+PCA search) ===\")\n",
    "for k, v in best_per_kernel.items():\n",
    "    print(f\"[{k}] RMSE_val={v['best_row']['rmse_val']:.6f} | params={v['best_params']}\")\n",
    "print(\"\\n=== Best overall (subset+PCA search) ===\")\n",
    "print(f\"Kernel: {best_kernel_name}\")\n",
    "print(\"Params:\", best_overall_params)\n",
    "\n",
    "# ---------- Final train on full Train+Val ----------\n",
    "from joblib import dump\n",
    "X_train_full = np.vstack([X_train, X_val]).astype(np.float32)\n",
    "y_train_full = np.concatenate([y_train, y_val]).astype(np.float32)\n",
    "\n",
    "if USE_PCA_FOR_FINAL:\n",
    "    # Fit scaler+PCA on full Train+Val, then SVR on reduced space\n",
    "    scaler_full = StandardScaler().fit(X_train_full)\n",
    "    Xtf = scaler_full.transform(X_train_full)\n",
    "    if PCA_COMPONENTS is not None:\n",
    "        pca_full = PCA(n_components=PCA_COMPONENTS, random_state=SEED).fit(Xtf)\n",
    "    else:\n",
    "        pca_probe = PCA(n_components=min(Xtf.shape), random_state=SEED).fit(Xtf)\n",
    "        cume = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "        k = int(np.searchsorted(cume, PCA_VAR_TARGET if PCA_VAR_TARGET else 0.99) + 1)\n",
    "        pca_full = PCA(n_components=k, random_state=SEED).fit(Xtf)\n",
    "        print(f\"[FINAL PCA] Target variance={PCA_VAR_TARGET or 0.99:.2f}; components={k}; \"\n",
    "              f\"explained={cume[k-1]:.4f}\")\n",
    "    Xtf_red = pca_full.transform(Xtf)\n",
    "\n",
    "    svr_kwargs = dict(C=best_overall_params[\"C\"], epsilon=best_overall_params[\"epsilon\"],\n",
    "                      kernel=best_overall_params[\"kernel\"], cache_size=CACHE_MB)\n",
    "    if \"gamma\"  in best_overall_params: svr_kwargs[\"gamma\"]  = best_overall_params[\"gamma\"]\n",
    "    if \"degree\" in best_overall_params: svr_kwargs[\"degree\"] = best_overall_params[\"degree\"]\n",
    "\n",
    "    final_model = SVR(**svr_kwargs)\n",
    "    t0 = time.perf_counter()\n",
    "    final_model.fit(Xtf_red, y_train_full)\n",
    "    print(f\"\\nFinal fit (WITH PCA) in {(time.perf_counter()-t0)/60:.2f} min.\")\n",
    "\n",
    "    # Test\n",
    "    X_test_f  = scaler_full.transform(X_test.astype(np.float32))\n",
    "    X_test_rf = pca_full.transform(X_test_f)\n",
    "    y_pred = final_model.predict(X_test_rf)\n",
    "\n",
    "    # Save artifacts\n",
    "    dump({\"scaler\": scaler_full, \"pca\": pca_full, \"svr\": final_model}, out / \"svr_with_pca.joblib\")\n",
    "else:\n",
    "    # Refit WITHOUT PCA: full pipeline = scale -> SVR on raw 2048D\n",
    "    final_pipe = Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"svr\", SVR(\n",
    "            C=best_overall_params[\"C\"],\n",
    "            epsilon=best_overall_params[\"epsilon\"],\n",
    "            kernel=best_overall_params[\"kernel\"],\n",
    "            gamma=best_overall_params.get(\"gamma\", \"scale\"),\n",
    "            degree=best_overall_params.get(\"degree\", 3),\n",
    "            cache_size=CACHE_MB\n",
    "        ))\n",
    "    ])\n",
    "    t0 = time.perf_counter()\n",
    "    final_pipe.fit(X_train_full, y_train_full)\n",
    "    print(f\"\\nFinal fit (NO PCA) in {(time.perf_counter()-t0)/60:.2f} min.\")\n",
    "    y_pred = final_pipe.predict(X_test.astype(np.float32))\n",
    "    dump(final_pipe, out / \"svr_pipeline_best.joblib\")\n",
    "\n",
    "# ---------- Metrics + parity plot ----------\n",
    "rmse_test = rmse(y_test, y_pred)\n",
    "mae_test  = mean_absolute_error(y_test, y_pred)\n",
    "r2_test   = r2_score(y_test, y_pred)\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"RMSE: {rmse_test:.6f}\")\n",
    "print(f\"MAE : {mae_test:.6f}\")\n",
    "print(f\"R^2 : {r2_test:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(y_test, y_pred, s=5, alpha=0.6)\n",
    "mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "plt.plot([mn, mx], [mn, mx], lw=2)\n",
    "plt.xlabel(\"True U0\"); plt.ylabel(\"Predicted U0\")\n",
    "plt.title(f\"SVR Parity (Test) — {best_kernel_name} (search via PCA)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- Save logs ----------\n",
    "all_df.to_csv(out / \"val_search_results_subset_pca_all.csv\", index=False)\n",
    "summary_rows = []\n",
    "for k, v in best_per_kernel.items():\n",
    "    br = v[\"best_row\"]\n",
    "    summary_rows.append({\n",
    "        \"kernel\": k, **v[\"best_params\"],\n",
    "        \"rmse_train\": br[\"rmse_train\"], \"rmse_val\": br[\"rmse_val\"],\n",
    "        \"mae_val\": br[\"mae_val\"], \"r2_val\": br[\"r2_val\"], \"secs\": br[\"secs\"]\n",
    "    })\n",
    "pd.DataFrame(summary_rows).to_csv(out / \"best_per_kernel_subset_pca.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b1ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search subset: 4000 / 103209\n",
      "\n",
      "--- Searching kernel: rbf ---\n",
      "Total combos: 18 | Already done: 0 | Remaining: 18\n",
      "[rbf] Batch 1/1 (18 configs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   3 out of  18 | elapsed:  5.1min remaining: 25.6min\n",
      "[Parallel(n_jobs=20)]: Done   5 out of  18 | elapsed:  5.2min remaining: 13.5min\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  18 | elapsed:  5.8min remaining:  9.1min\n",
      "[Parallel(n_jobs=20)]: Done   9 out of  18 | elapsed:  6.0min remaining:  6.0min\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  18 | elapsed:  6.2min remaining:  3.9min\n",
      "[Parallel(n_jobs=20)]: Done  13 out of  18 | elapsed:  6.3min remaining:  2.4min\n",
      "[Parallel(n_jobs=20)]: Done  15 out of  18 | elapsed:  6.4min remaining:  1.3min\n",
      "[Parallel(n_jobs=20)]: Done  18 out of  18 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rbf] rbf, C=300, eps=0.005, gamma=scale | val RMSE=879.611271 | 304.9s\n",
      "[rbf] rbf, C=300, eps=0.005, gamma=0.001 | val RMSE=978.098027 | 303.3s\n",
      "[rbf] rbf, C=300, eps=0.005, gamma=0.0003 | val RMSE=848.604794 | 309.5s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=scale | val RMSE=879.611369 | 310.5s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=0.001 | val RMSE=978.098104 | 309.1s\n",
      "[rbf] rbf, C=300, eps=0.01, gamma=0.0003 | val RMSE=848.604897 | 305.5s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=scale | val RMSE=820.151224 | 359.5s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=0.001 | val RMSE=937.045471 | 346.5s\n",
      "[rbf] rbf, C=600, eps=0.005, gamma=0.0003 | val RMSE=786.292332 | 368.9s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=scale | val RMSE=820.151489 | 359.8s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=0.001 | val RMSE=937.045645 | 350.3s\n",
      "[rbf] rbf, C=600, eps=0.01, gamma=0.0003 | val RMSE=786.292511 | 369.8s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=scale | val RMSE=797.498679 | 384.4s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=0.001 | val RMSE=914.390759 | 377.4s\n",
      "[rbf] rbf, C=900, eps=0.005, gamma=0.0003 | val RMSE=764.191319 | 397.4s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=scale | val RMSE=797.498929 | 387.3s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=0.001 | val RMSE=914.390978 | 379.3s\n",
      "[rbf] rbf, C=900, eps=0.01, gamma=0.0003 | val RMSE=764.191471 | 395.6s\n",
      "Best rbf: val RMSE=764.191319 | params={'kernel': 'rbf', 'C': 900, 'epsilon': 0.005, 'gamma': '0.0003'}\n",
      "\n",
      "=== Best per kernel (subset search) ===\n",
      "[rbf] RMSE_val=764.191319 | params={'kernel': 'rbf', 'C': 900, 'epsilon': 0.005, 'gamma': '0.0003'}\n",
      "\n",
      "=== Best overall (subset search) ===\n",
      "Kernel: rbf\n",
      "Params: {'kernel': 'rbf', 'C': 900, 'epsilon': 0.005, 'gamma': '0.0003'}\n",
      "\n",
      "[FINAL] Training on subset: 25000 / 116110\n",
      "\n",
      "[FINAL] Starting fit (exact RBF)...\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'gamma' parameter of SVR must be a str among {'auto', 'scale'} or a float in the range [0.0, inf). Got '0.0003' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 210\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[FINAL] Starting fit (exact RBF)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m t0 = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43mfinal_pipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m fit_mins = (time.perf_counter() - t0) / \u001b[32m60.0\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[FINAL] Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_mins\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colli\\anaconda3\\envs\\ml273a\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colli\\anaconda3\\envs\\ml273a\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colli\\anaconda3\\envs\\ml273a\\Lib\\site-packages\\sklearn\\base.py:1358\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1353\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1354\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1355\u001b[39m )\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colli\\anaconda3\\envs\\ml273a\\Lib\\site-packages\\sklearn\\base.py:471\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    465\u001b[39m \n\u001b[32m    466\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colli\\anaconda3\\envs\\ml273a\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'gamma' parameter of SVR must be a str among {'auto', 'scale'} or a float in the range [0.0, inf). Got '0.0003' instead."
     ]
    }
   ],
   "source": [
    "# svr_rbf_subset_final.py\n",
    "import os, time, json, math\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Search subset (no PCA for search)\n",
    "SUBSET_N = 4000           # 3k–5k is good; faster than 8k\n",
    "USE_PCA_FOR_SEARCH = False  # must be False for Option A\n",
    "\n",
    "# RBF-only, tiny focused grid (4–6 combos)\n",
    "param_grid_rbf = {\n",
    "    \"kernel\":  [\"rbf\"],\n",
    "    \"C\":       [300, 600, 900],         # push higher; 300 looked good\n",
    "    \"epsilon\": [0.005, 0.01],           # a bit tighter around best\n",
    "    \"gamma\":   [\"scale\", 1e-3, 3e-4],   # add mid-low between 1e-3 and 1e-4\n",
    "}\n",
    "# param_grid_rbf = {\n",
    "#     \"kernel\":  [\"rbf\"],\n",
    "#     \"C\":       [300, 600, 900],\n",
    "#     \"epsilon\": [0.01],\n",
    "#     \"gamma\":   [\"scale\", 1e-3],\n",
    "# }\n",
    "\n",
    "CACHE_MB = 4000           # bigger cache helps RBF\n",
    "N_JOBS = os.cpu_count() or 4\n",
    "BATCH = max(2, N_JOBS * 2)\n",
    "\n",
    "# Final training controls (subset to keep runtime sane)\n",
    "FINAL_USE_SUBSET = True\n",
    "FINAL_N = 25000          # train final exact RBF on 25k samples\n",
    "\n",
    "# ==================== DATA ASSUMPTION ====================\n",
    "# Assumes in-memory: X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ---------- Output dirs ----------\n",
    "out = Path(\"artifacts_svr\"); out.mkdir(exist_ok=True)\n",
    "logs_dir = out / \"logs\"; logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- Build training subset ----------\n",
    "if SUBSET_N is not None and SUBSET_N < len(X_train):\n",
    "    idx = np.random.choice(len(X_train), SUBSET_N, replace=False)\n",
    "    X_train_sub, y_train_sub = X_train[idx], y_train[idx]\n",
    "else:\n",
    "    X_train_sub, y_train_sub = X_train, y_train\n",
    "\n",
    "print(f\"Search subset: {len(X_train_sub)} / {len(X_train)}\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def param_row_key(params: dict) -> str:\n",
    "    return json.dumps(params, sort_keys=True)\n",
    "\n",
    "def append_row(path: Path, row: dict):\n",
    "    write_header = not path.exists()\n",
    "    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=write_header, index=False)\n",
    "\n",
    "def grid_to_param_list(grid: dict):\n",
    "    keys = list(grid.keys())\n",
    "    return [dict(zip(keys, vals)) for vals in product(*[grid[k] for k in keys])]\n",
    "\n",
    "def eval_one(params, X_tr, y_tr, X_va, y_va):\n",
    "    pipe = Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"svr\", SVR(\n",
    "            C=params[\"C\"], epsilon=params[\"epsilon\"],\n",
    "            kernel=params[\"kernel\"], gamma=params.get(\"gamma\", \"scale\"),\n",
    "            cache_size=CACHE_MB, tol=1e-3, shrinking=True\n",
    "        ))\n",
    "    ])\n",
    "    t0 = time.perf_counter()\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred_tr  = pipe.predict(X_tr)\n",
    "    pred_val = pipe.predict(X_va)\n",
    "    secs = time.perf_counter() - t0\n",
    "    return {\n",
    "        **params,\n",
    "        \"rmse_train\": rmse(y_tr, pred_tr),\n",
    "        \"rmse_val\": rmse(y_va, pred_val),\n",
    "        \"mae_val\": mean_absolute_error(y_va, pred_val),\n",
    "        \"r2_val\": r2_score(y_va, pred_val),\n",
    "        \"secs\": secs\n",
    "    }\n",
    "\n",
    "# ---------- Search spaces (RBF only) ----------\n",
    "search_spaces = { \"rbf\": param_grid_rbf }\n",
    "\n",
    "# ---------- Prepare search matrices (no PCA path) ----------\n",
    "X_train_sub_search = X_train_sub.astype(np.float32)\n",
    "X_val_search       = X_val.astype(np.float32)\n",
    "y_train_sub_search = y_train_sub.astype(np.float32)\n",
    "y_val_search       = y_val.astype(np.float32)\n",
    "\n",
    "# ---------- Chunked, parallel, resumable search ----------\n",
    "best_per_kernel = {}\n",
    "combined_csv = out / \"val_search_results_subset_streaming_rbf.csv\"\n",
    "# Start fresh for this run; prevents NaN kernel_group collisions\n",
    "if combined_csv.exists():\n",
    "    combined_csv.unlink()\n",
    "\n",
    "for kernel_name, grid in search_spaces.items():\n",
    "    print(f\"\\n--- Searching kernel: {kernel_name} ---\")\n",
    "    kernel_csv = logs_dir / f\"{kernel_name}_stream.csv\"\n",
    "\n",
    "    plist = grid_to_param_list(grid)\n",
    "    done_keys = set()\n",
    "    if kernel_csv.exists():\n",
    "        try:\n",
    "            prev = pd.read_csv(kernel_csv)\n",
    "            for _, row in prev.iterrows():\n",
    "                params = {k: row[k] for k in grid.keys() if k in row}\n",
    "                done_keys.add(param_row_key(params))\n",
    "        except Exception:\n",
    "            pass\n",
    "    todo = [p for p in plist if param_row_key(p) not in done_keys]\n",
    "    total = len(plist); remaining = len(todo)\n",
    "    print(f\"Total combos: {total} | Already done: {total-remaining} | Remaining: {remaining}\")\n",
    "\n",
    "    for start in range(0, remaining, BATCH):\n",
    "        chunk = todo[start:start+BATCH]\n",
    "        print(f\"[{kernel_name}] Batch {start//BATCH + 1}/{math.ceil(remaining/BATCH)} \"\n",
    "              f\"({len(chunk)} configs)...\")\n",
    "\n",
    "        results = Parallel(n_jobs=N_JOBS, backend=\"loky\", verbose=10, batch_size=1)(\n",
    "            delayed(eval_one)(\n",
    "                p,\n",
    "                X_train_sub_search, y_train_sub_search,\n",
    "                X_val_search,       y_val_search\n",
    "            ) for p in chunk\n",
    "        )\n",
    "\n",
    "        for row in results:\n",
    "            row_stream = dict(row); row_stream[\"kernel_group\"] = kernel_name\n",
    "            append_row(kernel_csv, row_stream)\n",
    "            append_row(combined_csv, row_stream)\n",
    "            print(f\"[{kernel_name}] {row.get('kernel')}, C={row.get('C')}, \"\n",
    "                  f\"eps={row.get('epsilon')}, gamma={row.get('gamma','-')} | \"\n",
    "                  f\"val RMSE={row['rmse_val']:.6f} | {row['secs']:.1f}s\")\n",
    "\n",
    "    df_k = pd.read_csv(kernel_csv).sort_values(\"rmse_val\").reset_index(drop=True)\n",
    "    best_row = df_k.iloc[0].to_dict()\n",
    "    best_params = {k: best_row[k] for k in grid.keys()}\n",
    "    best_per_kernel[kernel_name] = {\"best_row\": best_row, \"best_params\": best_params}\n",
    "    print(f\"Best {kernel_name}: val RMSE={best_row['rmse_val']:.6f} | params={best_params}\")\n",
    "\n",
    "# ---------- Best overall ----------\n",
    "all_df = pd.read_csv(combined_csv)\n",
    "# guard against junk/old rows\n",
    "all_df = all_df[all_df[\"kernel_group\"].notna()]\n",
    "all_df = all_df[all_df[\"kernel_group\"].isin(search_spaces.keys())]\n",
    "all_df = all_df.sort_values(\"rmse_val\").reset_index(drop=True)\n",
    "\n",
    "if len(all_df) == 0:\n",
    "    raise RuntimeError(\"No valid rows in combined_csv after filtering; check logs/ directory.\")\n",
    "\n",
    "best_overall_row = all_df.iloc[0].to_dict()\n",
    "best_kernel_name = best_overall_row[\"kernel_group\"]\n",
    "best_overall_params = {k: best_overall_row[k] for k in search_spaces[best_kernel_name].keys()}\n",
    "\n",
    "print(\"\\n=== Best per kernel (subset search) ===\")\n",
    "for k, v in best_per_kernel.items():\n",
    "    print(f\"[{k}] RMSE_val={v['best_row']['rmse_val']:.6f} | params={v['best_params']}\")\n",
    "print(\"\\n=== Best overall (subset search) ===\")\n",
    "print(f\"Kernel: {best_kernel_name}\")\n",
    "print(\"Params:\", best_overall_params)\n",
    "\n",
    "# ---------- Final train on Train+Val (optionally on a subset) ----------\n",
    "X_train_full = np.vstack([X_train, X_val]).astype(np.float32)\n",
    "y_train_full = np.concatenate([y_train, y_val]).astype(np.float32)\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"svr\", SVR(\n",
    "        C=best_overall_params[\"C\"],\n",
    "        epsilon=best_overall_params[\"epsilon\"],\n",
    "        kernel=best_overall_params[\"kernel\"],\n",
    "        gamma=best_overall_params.get(\"gamma\", \"scale\"),\n",
    "        degree=best_overall_params.get(\"degree\", 3),\n",
    "        cache_size=CACHE_MB, tol=1e-3, shrinking=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Choose subset (25k) or full\n",
    "if FINAL_USE_SUBSET:\n",
    "    n_all = len(X_train_full)\n",
    "    n_final = min(FINAL_N, n_all)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    idx_final = rng.choice(n_all, n_final, replace=False)\n",
    "    X_final = X_train_full[idx_final]\n",
    "    y_final = y_train_full[idx_final]\n",
    "    pd.DataFrame({\"idx_final\": idx_final}).to_csv(out / f\"final_subset_indices_{n_final}.csv\", index=False)\n",
    "    print(f\"\\n[FINAL] Training on subset: {n_final} / {n_all}\")\n",
    "else:\n",
    "    X_final, y_final = X_train_full, y_train_full\n",
    "    print(f\"\\n[FINAL] Training on FULL Train+Val: {len(X_final)}\")\n",
    "\n",
    "print(\"\\n[FINAL] Starting fit (exact RBF)...\")\n",
    "t0 = time.perf_counter()\n",
    "final_pipe.fit(X_final, y_final)\n",
    "fit_mins = (time.perf_counter() - t0) / 60.0\n",
    "print(f\"[FINAL] Done in {fit_mins:.2f} min.\")\n",
    "\n",
    "# ---------- Test evaluation ----------\n",
    "y_pred = final_pipe.predict(X_test.astype(np.float32))\n",
    "rmse_test = rmse(y_test, y_pred)\n",
    "mae_test  = mean_absolute_error(y_test, y_pred)\n",
    "r2_test   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Test Performance (Final, subset={}) ===\".format(FINAL_USE_SUBSET))\n",
    "print(f\"RMSE: {rmse_test:.6f}\")\n",
    "print(f\"MAE : {mae_test:.6f}\")\n",
    "print(f\"R^2 : {r2_test:.6f}\")\n",
    "\n",
    "# ---------- Parity plot ----------\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(y_test, y_pred, s=5, alpha=0.6)\n",
    "mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "plt.plot([mn, mx], [mn, mx], lw=2)\n",
    "plt.xlabel(\"True U0\"); plt.ylabel(\"Predicted U0\")\n",
    "plt.title(f\"SVR Parity (Test) — RBF, subset={FINAL_USE_SUBSET}\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- Save artifacts ----------\n",
    "all_df.to_csv(out / \"val_search_results_subset_rbf_all.csv\", index=False)\n",
    "summary_rows = []\n",
    "for k, v in best_per_kernel.items():\n",
    "    br = v[\"best_row\"]\n",
    "    summary_rows.append({\n",
    "        \"kernel\": k, **v[\"best_params\"],\n",
    "        \"rmse_train\": br[\"rmse_train\"], \"rmse_val\": br[\"rmse_val\"],\n",
    "        \"mae_val\": br[\"mae_val\"], \"r2_val\": br[\"r2_val\"], \"secs\": br[\"secs\"]\n",
    "    })\n",
    "pd.DataFrame(summary_rows).to_csv(out / \"best_per_kernel_subset_rbf.csv\", index=False)\n",
    "\n",
    "dump(final_pipe, out / (\"svr_pipeline_final_subset.joblib\" if FINAL_USE_SUBSET else \"svr_pipeline_final_full.joblib\"))\n",
    "pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred}).to_csv(out / \"test_preds_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03848bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL] Training on subset: 25000 / 116110\n",
      "\n",
      "[FINAL] Starting fit (exact RBF on 25k)...\n",
      "[FINAL] Done in 51.27 min.\n",
      "\n",
      "=== Test Performance (RBF, 25k subset) ===\n",
      "RMSE: 628.537507\n",
      "MAE : 401.022466\n",
      "R^2 : 0.668534\n",
      "[SAVE] Model -> artifacts_svr\\svr_pipeline_final_subset25k.joblib\n",
      "[SAVE] Predictions -> artifacts_svr\\test_preds_final_subset25k.csv\n"
     ]
    }
   ],
   "source": [
    "# === Final exact RBF SVR on a 25k subset (reproducible) ===\n",
    "import time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from joblib import dump\n",
    "\n",
    "# ---- Best params from your search (with gamma string -> float fix) ----\n",
    "best_params = {\"kernel\": \"rbf\", \"C\": 900, \"epsilon\": 0.005, \"gamma\": \"0.0003\"}\n",
    "gamma_val = best_params[\"gamma\"]\n",
    "if isinstance(gamma_val, str) and gamma_val not in (\"scale\", \"auto\"):\n",
    "    try:\n",
    "        gamma_val = float(gamma_val)\n",
    "    except Exception:\n",
    "        gamma_val = \"scale\"\n",
    "\n",
    "# ---- Build Train+Val, pick reproducible 25k subset ----\n",
    "SEED = 42\n",
    "FINAL_N = 25_000\n",
    "out = Path(\"artifacts_svr\"); out.mkdir(exist_ok=True)\n",
    "\n",
    "X_train_full = np.vstack([X_train, X_val]).astype(np.float32)\n",
    "y_train_full = np.concatenate([y_train, y_val]).astype(np.float32)\n",
    "n_all = len(X_train_full)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx_final = rng.choice(n_all, min(FINAL_N, n_all), replace=False)\n",
    "X_final = X_train_full[idx_final]\n",
    "y_final = y_train_full[idx_final]\n",
    "\n",
    "# save indices for reproducibility\n",
    "pd.DataFrame({\"idx_final\": idx_final}).to_csv(out / f\"final_subset_indices_{len(idx_final)}.csv\", index=False)\n",
    "print(f\"[FINAL] Training on subset: {len(idx_final)} / {n_all}\")\n",
    "\n",
    "# ---- Pipeline and fit (timed) ----\n",
    "final_pipe = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"svr\", SVR(\n",
    "        C=best_params[\"C\"],\n",
    "        epsilon=best_params[\"epsilon\"],\n",
    "        kernel=best_params[\"kernel\"],\n",
    "        gamma=gamma_val,\n",
    "        cache_size=4000,\n",
    "        tol=1e-3,\n",
    "        shrinking=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n[FINAL] Starting fit (exact RBF on 25k)...\")\n",
    "t0 = time.perf_counter()\n",
    "final_pipe.fit(X_final, y_final)\n",
    "mins = (time.perf_counter() - t0) / 60.0\n",
    "print(f\"[FINAL] Done in {mins:.2f} min.\")\n",
    "\n",
    "# ---- Test evaluation ----\n",
    "X_test_f = X_test.astype(np.float32)\n",
    "y_pred = final_pipe.predict(X_test_f)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "mae  = float(mean_absolute_error(y_test, y_pred))\n",
    "r2   = float(r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== Test Performance (RBF, 25k subset) ===\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE : {mae:.6f}\")\n",
    "print(f\"R^2 : {r2:.6f}\")\n",
    "\n",
    "# ---- Save artifacts ----\n",
    "dump(final_pipe, out / \"svr_pipeline_final_subset25k.joblib\")\n",
    "pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred}).to_csv(out / \"test_preds_final_subset25k.csv\", index=False)\n",
    "print(f\"[SAVE] Model -> {out/'svr_pipeline_final_subset25k.joblib'}\")\n",
    "print(f\"[SAVE] Predictions -> {out/'test_preds_final_subset25k.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92182bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on subset: 4000 samples\n",
      "=== Training subset performance (4000 samples) ===\n",
      "RMSE: 531.0762\n",
      "MAE : 223.6871\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# Load the same 25k subset indices\n",
    "# -------------------------------\n",
    "idx_path = \"artifacts_svr/final_subset_indices_25000.csv\"  # adjust if named differently\n",
    "idx_final = pd.read_csv(idx_path)[\"idx_final\"].values\n",
    "\n",
    "# -------------------------------\n",
    "# Build the 25k training pool\n",
    "# -------------------------------\n",
    "X_train_full = np.vstack([X_train, X_val]).astype(np.float32)\n",
    "y_train_full = np.concatenate([y_train, y_val]).astype(np.float32)\n",
    "\n",
    "X_pool = X_train_full[idx_final]\n",
    "y_pool = y_train_full[idx_final]\n",
    "\n",
    "# -------------------------------\n",
    "# Choose 4000-sample training subset\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "N_SMALL = 4000\n",
    "\n",
    "subset_idx = rng.choice(len(X_pool), N_SMALL, replace=False)\n",
    "X_small = X_pool[subset_idx]\n",
    "y_small = y_pool[subset_idx]\n",
    "\n",
    "print(f\"Training on subset: {len(X_small)} samples\")\n",
    "\n",
    "# -------------------------------\n",
    "# Define best SVR model\n",
    "# -------------------------------\n",
    "best_params = {\"kernel\": \"rbf\", \"C\": 900, \"epsilon\": 0.005, \"gamma\": 3e-4}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"svr\", SVR(\n",
    "        C=best_params[\"C\"],\n",
    "        epsilon=best_params[\"epsilon\"],\n",
    "        kernel=best_params[\"kernel\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        cache_size=4000,\n",
    "        tol=1e-3\n",
    "    ))\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Fit and evaluate on training subset\n",
    "# -------------------------------\n",
    "pipe.fit(X_small, y_small)\n",
    "y_pred_train = pipe.predict(X_small)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_small, y_pred_train))\n",
    "mae_train  = mean_absolute_error(y_small, y_pred_train)\n",
    "\n",
    "print(\"=== Training subset performance (4000 samples) ===\")\n",
    "print(f\"RMSE: {rmse_train:.4f}\")\n",
    "print(f\"MAE : {mae_train:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml273a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
